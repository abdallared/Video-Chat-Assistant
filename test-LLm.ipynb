{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad7cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Temp\\ipykernel_25976\\4048587293.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.2,\n",
    "    num_predict=300     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0145bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ù†Ø§ Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„Ù‹Ø§ Ù…ØµÙ…Ù…Ù‹Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø´Ø±ÙƒØ© MetaØŒ ØµÙÙ…Ù… Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£ØªØ¹Ù„Ù… Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ Ø£ØµØ¨Ø­Øª Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§. ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«ÙŠ Ø¯ÙˆØ±ÙŠØ§Ù‹ Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚ØªÙ‡Ø§. Ø¥Ù†Ù‡ Ø¹Ù…Ù„ Ø¬Ù‡Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ù‚Ø¨Ù„ ÙØ±ÙŠÙ‚ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ù…ØµÙ…Ù…ÙŠÙ† ÙˆØ§Ù„Ù…ØªØ®ØµØµÙŠÙ† ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke('Ø¹Ø±Ù Ø¹Ù† Ù†ÙØ³Ùƒ')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9070f",
   "metadata": {},
   "source": [
    "## faiss DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5684e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Temp\\ipykernel_25976\\4221662540.py:21: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def normalize(vectors):\n",
    "    return [v / np.linalg.norm(v) if np.linalg.norm(v) != 0 else v for v in vectors]\n",
    "\n",
    "\n",
    "folder = r\"./transcriptions\"\n",
    "docs = []\n",
    "for fn in os.listdir(folder):\n",
    "    if fn.endswith(\".txt\"):\n",
    "        for d in TextLoader(os.path.join(folder, fn), encoding=\"utf-8\").load():\n",
    "            d.metadata = {\"source\": fn}\n",
    "            docs.append(d)\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "raw_embeddings = embedding.embed_documents([doc.page_content for doc in docs])\n",
    "normalized_embeddings = normalize(raw_embeddings)\n",
    "\n",
    "text_embedding_pairs = [\n",
    "    (doc.page_content, embedding_vec)\n",
    "    for doc, embedding_vec in zip(docs, normalized_embeddings)\n",
    "]\n",
    "\n",
    "vectordb = FAISS.from_embeddings(text_embedding_pairs, embedding)\n",
    "\n",
    "vectordb.save_local(r\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f696a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82cb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(vectors):\n",
    "    return [v / np.linalg.norm(v) if np.linalg.norm(v) != 0 else v for v in vectors]\n",
    "\n",
    "def query_faiss(question: str, k: int = 3, threshold: float = 0.7):\n",
    "    query_embedding = embedding.embed_query(question)\n",
    "    normalized_query = normalize([query_embedding])[0]\n",
    "\n",
    "    docs_and_scores = vectordb.similarity_search_with_score_by_vector(normalized_query, k=k)\n",
    "\n",
    "    results = []\n",
    "    for doc, score in docs_and_scores:\n",
    "        results.append({\n",
    "            \"text\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": score \n",
    "        })\n",
    "\n",
    "    filtered = [r for r in results if r[\"score\"] >= threshold]\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb05ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cosine = 0.6063 | Source: None\n",
      "Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª. Ø®Ù„ÙŠ Ø¨Ø§Ù„Ùƒ Ø§ÙˆÙŠ Ù…Ù† ÙƒÙ„Ù…Ø© Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø¯ÙŠ. Ù…Ø¶Ø§Ø¯Ø©. Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª. Ø§Ù„Ø¯Ù‚ÙŠÙ‚. ØªØ§Ù†ÙŠØ© ÙƒØ§Ù† Ø§Ø³Ù…Ù‡Ø§ Ù…ÙˆØ§Ø¯ ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ© Ù… \n",
      "---\n",
      "\n",
      "2. cosine = 0.6134 | Source: None\n",
      "Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©. Ø§Ø­Ù†Ø§ Ù…Ø´ Ø¹Ø§ÙŠØ²ÙŠÙ† Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯ Ø§Ù„ÙƒØ¨ÙŠØ±. Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ \n",
      "---\n",
      "\n",
      "3. cosine = 0.6175 | Source: None\n",
      "Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª. Ø§ØªØ¶Ù‚Ù‚. Ø§Ø²Ø§ Ø§Ù†Øª ÙƒØ¯Ù‡ ÙŠØ¹ØªØ¨Ø± Ø®Ù„Ø§Øµ Ø®Ù„Ø§ØµÙ†Ø§. ÙŠØ¨Ù‚Ù‰ Ø«Ø§Ù†ÙŠØ© Ø§ÙˆÙ„Ø§ Ø§Ø­Ù†Ø§ Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø¨Ù†ØªÙƒÙ„Ù… ÙÙŠ \n",
      "---\n",
      "\n",
      "4. cosine = 0.6268 | Source: None\n",
      "ØªØ§Ù†ÙŠ. Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Øª ÙƒØ¯Ù‡ ÙƒØ¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ùˆ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ© ÙŠØ¹Ù†ÙŠ Ø§Ù„Ù„ÙŠ Ù‡ÙŠ Ù…Ø¶Ø§Ø¯Ø© Ø§Ù„ \n",
      "---\n",
      "\n",
      "5. cosine = 0.6566 | Source: None\n",
      "Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¶Ù‚ÙŠÙ‚Ø©. Ø·ÙŠØ¨ ØªØ¹Ø§Ù„Ù‰ Ø¨Ù‚Ù‰ ÙƒØ¯Ù‡ ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙ†Ø´ÙˆÙ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Øª. Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Øª ÙŠØ¹Ù†ÙŠ \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡'\n",
    "results = query_faiss(query, k=5, threshold=0.6)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. cosine = {r['score']:.4f} | Source: {r['metadata'].get('source')}\")\n",
    "    print(r['text'][:100], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e782cc",
   "metadata": {},
   "source": [
    "## Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083f9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Temp\\ipykernel_25976\\1833664569.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(#history\n",
      "E:\\Temp\\ipykernel_25976\\1833664569.py:39: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm, prompt=qa_prompt),\n",
      "E:\\Temp\\ipykernel_25976\\1833664569.py:38: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  qa_chain_stuff = StuffDocumentsChain(\n",
      "E:\\Temp\\ipykernel_25976\\1833664569.py:46: LangChainDeprecationWarning: The class `ConversationalRetrievalChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~create_history_aware_retriever together with create_retrieval_chain (see example in docstring)` instead.\n",
      "  qa_chain = ConversationalRetrievalChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "\n",
    "condense_template = PromptTemplate.from_template(\n",
    "    \"Given the conversation below, rewrite the user's last question as a standalone question. \"\n",
    "    \"Important: write it in the SAME language as the user asked.\\n\\n\"\n",
    "    \"Conversation:\\n{chat_history}\\n\\n\"\n",
    "    \"Standalone question:\"\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(#history \n",
    "    llm=llm,                  \n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True,     \n",
    "    buffer_size=3,          \n",
    "    max_token_limit=1000\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful tutor for school students. \n",
    "    Always explain in a simple way that a child can understand. \n",
    "    Use clear examples, and avoid difficult academic words.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer in the same language of the question:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_chain_stuff = StuffDocumentsChain(\n",
    "    llm_chain=LLMChain(llm=llm, prompt=qa_prompt),\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=qa_chain_stuff,\n",
    "    question_generator=LLMChain(llm=llm, prompt=condense_template),\n",
    "    memory=memory,\n",
    "    return_source_documents=False,\n",
    "    verbose=False,\n",
    "    max_tokens_limit=300,\n",
    "    output_key=\"answer\"  # â† Ø¯Ù‡ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ù„ÙŠ Ù‡ÙŠØªØ®Ø²Ù† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a6166ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I\\'ll do my best to translate the new line of conversation.\\n\\nCurrent summary:\\nThe human asks about proteins that fight against microscopic organisms. The AI explains that these proteins are a necessary substance, and they work to kill microscopic entities like bacteria and viruses. For example, when you feel itchy after using the bathroom, it might be due to your body producing proteins that fight off bacteria causing the itch.\\n\\nNew lines of conversation:\\nHuman: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡ (What are the proteins that fight against microscopic organisms?)\\n\\nNew summary:\\nThe human asks about proteins that fight against microscopic organisms. The AI explains that these proteins are a necessary substance, and they work to kill microscopic entities like bacteria and viruses. For example, when you feel itchy after using the bathroom, it might be due to your body producing proteins that fight off bacteria causing the itch.\\n\\n(Note: I translated \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡\" from Arabic to English as \"What are the proteins that fight against microscopic organisms?\")' additional_kwargs={} response_metadata={}\n",
      "content='Ø­Ø³Ù†Ø§Ù‹ ÙŠØ§ ØµØºÙŠØ±ØªÙŠ/ØµØºÙŠØ±ÙŠ!\\n\\nØ§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù‡ÙŠ Ù…Ø«Ù„ Ø­Ø§Ø±Ø³ Ø§Ù„Ø£Ù…Ù† ÙÙŠ Ø¬Ø³Ù…Ùƒ. ÙˆÙ‡ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø¦Ù…Ù‹Ø§ ÙÙŠ Ø¬Ø³Ù…ÙƒØŒ Ø­ØªÙ‰ Ù‚Ø¨Ù„ Ø£Ù† ØªØªØ¹Ø±Ø¶ Ù„Ù…Ø±Ø¶ Ù…Ø§.\\n\\nÙ…Ø«Ù„Ù‹Ø§ØŒ Ø¹Ù†Ø¯Ù…Ø§ ØªØ£ÙƒÙ„ÙŠÙ† Ø§Ù„Ø·Ø¹Ø§Ù…ØŒ ÙŠØ£ØªÙŠ Ù…Ø¹Ù‡ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø§Ù„ØµØºÙŠØ±Ø©. ÙˆÙ„ÙƒÙ† Ø§Ù„Ø¬Ø³Ù… Ù„Ø¯ÙŠÙ‡ Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ù‚ØªÙ„ Ù‡Ø°Ù‡ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ù‚Ø¨Ù„ Ø£Ù† ØªØ³Ø¨Ø¨ Ø£ÙŠ Ø¶Ø±Ø±.\\n\\nÙÙƒØ± ÙÙŠ Ø°Ù„Ùƒ Ù…Ø«Ù„ Ø­Ø§Ø±Ø³ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø°ÙŠ ÙŠØ­Ø±Ø³ Ø¨Ø§Ø¨ Ø§Ù„Ù…Ù†Ø²Ù„ Ù…Ù† Ø£ÙŠ Ø´Ø®Øµ ØºÙŠØ± Ù…Ø±ØºÙˆØ¨ ÙÙŠÙ‡. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙƒØ§Ø¦Ù† Ø¯Ù‚ÙŠÙ‚ ÙŠØ±ÙŠØ¯ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø¬Ø³Ù…ÙƒØŒ Ø³ÙˆÙ ÙŠÙ‚ØªÙ„ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù‡Ø°Ø§ Ø§Ù„ÙƒØ§Ø¦Ù† Ù‚Ø¨Ù„ Ø£Ù† ÙŠØ³Ø¨Ø¨ Ø£ÙŠ Ø¶Ø±Ø±.\\n\\nÙ‡Ø°Ù‡ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø¦Ù…Ù‹Ø§ ÙÙŠ Ø¬Ø³Ù…ÙƒØŒ Ø­ØªÙ‰ Ù‚Ø¨Ù„ Ø£Ù† ØªØªØ¹Ø±Ø¶ Ù„Ù…Ø±Ø¶ Ù…Ø§. ÙˆÙ‡Ù… ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¹Ù„Ù‰ Ø­Ù…Ø§ÙŠØ© Ø¬Ø³Ù…Ùƒ Ù…Ù† Ø£ÙŠ ÙƒØ§Ø¦Ù† Ø¯Ù‚ÙŠÙ‚ Ù‚Ø¯ ÙŠØ±ÙŠØ¯ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„ÙŠÙ‡.' additional_kwargs={} response_metadata={}\n",
      "content='Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡' additional_kwargs={} response_metadata={}\n",
      "content='' additional_kwargs={} response_metadata={}\n",
      "content='Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡' additional_kwargs={} response_metadata={}\n",
      "content='' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})[\"chat_history\"][0])\n",
    "print(memory.load_memory_variables({})[\"chat_history\"][1])\n",
    "print(memory.load_memory_variables({})[\"chat_history\"][2])\n",
    "print(memory.load_memory_variables({})[\"chat_history\"][3])\n",
    "print(memory.load_memory_variables({})[\"chat_history\"][4])\n",
    "print(memory.load_memory_variables({})[\"chat_history\"][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2df07a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_reply_from_vector(question: str, chat_history=None, top_k_sources=3):\n",
    "    search = query_faiss(question, top_k_sources, threshold=0.7)\n",
    "    if not search:\n",
    "        return None\n",
    "\n",
    "    inputs = {\"question\": question}\n",
    "    if chat_history:\n",
    "        inputs[\"chat_history\"] = chat_history\n",
    "\n",
    "    result = qa_chain(inputs)\n",
    "\n",
    "    answer = result.get(\"answer\") or result.get(\"result\") or result.get(\"output_text\") or \"\"\n",
    "\n",
    "    sources = []\n",
    "    for item in search[:top_k_sources]:\n",
    "        sources.append({\n",
    "            \"text\": item[\"text\"],\n",
    "            \"metadata\": item.get(\"metadata\", {})\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"source\": \"video\",\n",
    "        \"answer\": answer.strip(),\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0319",
   "metadata": {},
   "source": [
    "## serch from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def qa_chain_web(question: str, web_results: list, chat_history=None):\n",
    "    docs = []\n",
    "    links = []\n",
    "\n",
    "    for r in web_results:\n",
    "        body = r.get(\"body\", \"\")\n",
    "        href = r.get(\"href\", \"\")\n",
    "\n",
    "        if body.strip() and href.strip():\n",
    "            docs.append(Document(page_content=body.strip()))\n",
    "            links.append(href.strip())\n",
    "\n",
    "    if not docs:\n",
    "        return {\n",
    "            \"source\": \"internet\",\n",
    "            \"answer\": \"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "\n",
    "    if chat_history:\n",
    "        memory.save_context({\"input\": question}, {\"output\": \"\"})\n",
    "\n",
    "    answer = qa_chain_stuff.run({\n",
    "        \"input_documents\": docs,\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"source\": \"internet\",\n",
    "        \"answer\": answer.strip(),\n",
    "        \"sources\": links  # â† Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from ddgs import DDGS\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v / norm if norm != 0 else v\n",
    "\n",
    "embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "def rerank_web_results(question, max_results=5, threshold=0.65):\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(question, max_results=max_results))\n",
    "\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    q_emb = normalize(embedding_function.embed_query(question))\n",
    "    filtered = []\n",
    "\n",
    "    for r in results:\n",
    "        body = r.get(\"body\", \"\")\n",
    "        href = r.get(\"href\", \"\")\n",
    "\n",
    "        if not body.strip() or not href:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            d_emb = normalize(embedding_function.embed_query(body))\n",
    "            sim = cosine_similarity(\n",
    "                np.array(q_emb).reshape(1, -1),\n",
    "                np.array(d_emb).reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            if sim >= threshold:\n",
    "                filtered.append({\n",
    "                    \"body\": body.strip(),\n",
    "                    \"href\": href.strip()\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcbedf",
   "metadata": {},
   "source": [
    "## the final chat-with-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd5de2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer from web\n",
      "Ø­Ø³Ù†Ø§Ù‹ ÙŠØ§ Ø·Ø§Ù„Ø¨! Ø³Ø£Ø´Ø±Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨Ø³Ù‡ÙˆÙ„Ø©.\n",
      "\n",
      "Ø§Ù„Ø¬Ù…Ù„Ø© ØªÙ‚ÙˆÙ„: \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©ØŸ\"\n",
      "\n",
      "Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ Ù…Ø¹Ø±ÙØ© Ù…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ Ø¶Ø¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¨ÙƒØªÙŠØ±ÙŠØ§ ÙˆØ§Ù„ÙÙŠØ±ÙˆØ³Ø§Øª.\n",
      "\n",
      "Ø¨Ø§Ø®ØªØµØ§Ø±ØŒ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ù‡ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ ØªØ­Ù…ÙŠ Ø§Ù„Ø¬Ø³Ù… Ù…Ù† Ø§Ù„Ø¹Ø¯ÙˆÙ‰ ÙˆØ§Ù„Ù…Ø±Ø¶.\n",
      "\n",
      "ÙÙ‡Ù…Øª Ø¬ÙŠØ¯Ø§Ù‹ØŸ\n",
      "['https://mawdoo3.com/Ø£Ù†ÙˆØ§Ø¹_Ù…Ø§_ÙÙŠ_Ø§Ù„Ù„ØºØ©_Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'https://loghate.com/s/Ù…Ø§-Ù‡ÙŠ-Ø£Ù†ÙˆØ§Ø¹-Ù…Ø§-ÙˆØ¥Ø¹Ø±Ø§Ø¨Ù‡Ø§']\n"
     ]
    }
   ],
   "source": [
    "q = 'Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø¶Ø§Ø¶Ù‡ Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ù‡'\n",
    "out = first_reply_from_vector(q)\n",
    "if out is None:\n",
    "    print(\"answer from web\")\n",
    "    results = rerank_web_results(q,5)\n",
    "    if not results:\n",
    "        print('no ans')\n",
    "    else:\n",
    "        web_result = qa_chain_web(q,results,memory)##from net and llm\n",
    "        print(web_result['answer'])\n",
    "        print(web_result['sources'])\n",
    "else:\n",
    "    print('answer from video')\n",
    "    print(out[\"answer\"])\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.12.7 environment with all requirements installed successfully!\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"All packages from requirements.txt are installed and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd23d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9164d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafe155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4675651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app1.py\n",
    "import streamlit as st\n",
    "import time\n",
    "\n",
    "# --- Page setup ---\n",
    "st.set_page_config(page_title=\"Video Chat\", page_icon=\"ğŸ¥\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    .block-container {padding-top: 1rem; padding-bottom: 1rem;}\n",
    "    .stChatInput {position: fixed; bottom: 20px; right: 2%; width: 28% !important;}\n",
    "    .stChatMessage {max-width: 90%;}\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# --- Initialize session state ---\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"processing\" not in st.session_state:\n",
    "    st.session_state.processing = False\n",
    "\n",
    "# --- MAIN SECTION ---\n",
    "st.header(\"ğŸ¥ Video Chat\")\n",
    "\n",
    "# --- Sidebar with clear chat button ---\n",
    "with st.sidebar:\n",
    "    st.header(\"ğŸ¥ Video Chat\")\n",
    "    st.write(f\"Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„: {len(st.session_state.messages)}\")\n",
    "    if st.button(\"ğŸ—‘ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\"):\n",
    "        st.session_state.messages = []\n",
    "        st.success(\"ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©!\")\n",
    "        st.rerun()\n",
    "\n",
    "# --- Layout ---\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\")\n",
    "    st.video(\"https://youtu.be/-zHgacHhS5Q\")\n",
    "    st.write(\"ÙÙŠØ¯ÙŠÙˆ ØªØ¬Ø±ÙŠØ¨ÙŠ ÙŠØ¹Ù…Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­ÙŠØ©\")\n",
    "    chat_container = st.container()\n",
    "    with chat_container:\n",
    "        for message in st.session_state.messages:\n",
    "            with st.chat_message(message[\"role\"]):\n",
    "                st.markdown(message[\"content\"])\n",
    "\n",
    "# --- Chat input ---\n",
    "if not st.session_state.processing:\n",
    "    if prompt := st.chat_input(\"Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø£Ùˆ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹...\"):\n",
    "        st.session_state.processing = True\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        with chat_container:\n",
    "            with st.chat_message(\"user\"):\n",
    "                st.markdown(prompt)\n",
    "\n",
    "        # --- Ø§Ù„Ø±Ø¯ Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø£Ùˆ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ---\n",
    "        q = prompt\n",
    "        out = first_reply_from_vector(q)\n",
    "\n",
    "        if out is None:\n",
    "            results = rerank_web_results(q, 5)\n",
    "            if not results:\n",
    "                assistant_response = \"âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.\"\n",
    "            else:\n",
    "                web_result = qa_chain_web(q, results, memory)\n",
    "                answer = web_result['answer']\n",
    "                sources = web_result['sources']\n",
    "\n",
    "                assistant_response = answer.strip() + \"\\n\\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±:\\n\"\n",
    "                for i, link in enumerate(sources, 1):\n",
    "                    assistant_response += f\"{i}. [Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±]({link})\\n\"\n",
    "        else:\n",
    "            assistant_response = out[\"answer\"].strip() + \"\\n\\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ.\"\n",
    "\n",
    "        # --- Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯ ---\n",
    "        with chat_container:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                message_placeholder = st.empty()\n",
    "                full_response = \"\"\n",
    "                for chunk in assistant_response.split():\n",
    "                    full_response += chunk + \" \"\n",
    "                    time.sleep(0.05)\n",
    "                    message_placeholder.markdown(full_response + \"â–Œ\")\n",
    "                message_placeholder.markdown(full_response)\n",
    "\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        st.session_state.processing = False\n",
    "        st.rerun()\n",
    "\n",
    "# --- Footer ---\n",
    "st.divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'streamlet' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4690589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run app1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78eceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3127",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
